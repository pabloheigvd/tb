{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/c/c7/HEIG-VD_Logo_96x29_RVB_ROUGE.png\" alt=\"HEIG-VD Logo\" width=\"250\"/>\n",
    "\n",
    "# TB - DevOps: Mise en place de cloud-native storage\n",
    "## fio benchmark\n",
    "**Note**: La version de fio utilisée pour chaque benchmark va être explicitement décrite tel que la license morale (section 4 [documentation](https://fio.readthedocs.io/en/latest/fio_doc.html#moral-license) fio) nous recommande."
   ]
  },
  {
   "source": [
    "Les tests se sont basés sur le benchmarking réalisé par [Architecting-IT](https://resources.storageos.com/downloadbenchmarkreport). Le benchmarking des différentes solutions ont réussi à démontré que StorageOs ressortait en tête parmi ses principaux concurrent en tant que solution de stockage block.\n",
    "\n",
    "Ce que l'on va tenter de montrer est les différentes options proposées par Longhorn, essayer de comprendre son fonctionnement interne et voir si cela se manifeste dans notre benchmarking. \n",
    "\n",
    "Dans la méthodologie de test, on va montrer les différents déploiements utilisés (ce qui n'est pas fait dans le rapport par Architecting-IT) et proposer un déploiement via une image, ce qui aidera à la reproductibilité des tests effectués. Architecting-IT a effectué ses tests manuellement pour les raisons suivantes:\n",
    "* To ensure that any lazy write process on file system creation didn’t affect the performance figures.\n",
    "* To permit the validation of the container<->volume mapping and check the volume was the requisite size.\n",
    "* To run iostat, top and iftop on each worker node. The output from these commands is not included but used to validate that there are no bottlenecks in CPU and network utilisation that could affect performance during test runs.\n",
    "\n",
    "TODO demander à Marcel Graf si c'est grave? Est-ce que je devrais moi aussi run iostat+top+iftop pour guarantir qu'il n'y ait pas de CPU ou network bottleneck? \n",
    "\n",
    "On souhaite explorer les différents paramètres que Longhorn offre et voir leurs impacts dans les performances. Le hardware utilisé étant différent, on peut s'attendre à des résultats différents."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Test en Local\n",
    "Les résultats de cette section ont été généré à l'aide de fio version 3.16 à l'aide du script `run-all-jobs.sh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "# from: https://stackoverflow.com/a/3207973\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "fio_jobs_path = 'fio-jobs-output/local'\n",
    "job_outputs_filename = [f for f in listdir(fio_jobs_path) if isfile(join(fio_jobs_path, f)) and 'output' in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fio version': 'fio-3.16', 'timestamp': 1624716221, 'timestamp_ms': 1624716221209, 'time': 'Sat Jun 26 16:03:41 2021', 'global options': {'bs': '4K', 'iodepth': '256', 'direct': '1', 'ioengine': 'libaio', 'runtime': '120', 'numjobs': '4', 'size': '512M', 'name': 'raw-seqread', 'rw': 'read'}, 'jobs': [{'jobname': 'job1', 'groupid': 0, 'error': 0, 'eta': 0, 'elapsed': 121, 'job options': {'filename': '/dev/nvme0n1p7'}, 'read': {'io_bytes': 85691441152, 'io_kbytes': 83683048, 'bw_bytes': 714035839, 'bw': 697300, 'iops': 174325.156237, 'runtime': 120010, 'total_ios': 20920762, 'short_ios': 0, 'drop_ios': 0, 'slat_ns': {'min': 712, 'max': 318915, 'mean': 1345.356346, 'stddev': 891.144287}, 'clat_ns': {'min': 93938, 'max': 19756113, 'mean': 5872300.887986, 'stddev': 2146245.617705, 'percentile': {'1.000000': 1482752, '5.000000': 4014080, '10.000000': 4145152, '20.000000': 4227072, '30.000000': 4358144, '40.000000': 4489216, '50.000000': 5144576, '60.000000': 5406720, '70.000000': 7569408, '80.000000': 8028160, '90.000000': 9371648, '95.000000': 9895936, '99.000000': 11075584, '99.500000': 11599872, '99.900000': 12779520, '99.950000': 13303808, '99.990000': 14483456}}, 'lat_ns': {'min': 96030, 'max': 19757323, 'mean': 5873719.170813, 'stddev': 2146249.697853}, 'bw_min': 456392, 'bw_max': 940271, 'bw_agg': 99.999382, 'bw_mean': 697295.6875, 'bw_dev': 40813.563058, 'bw_samples': 960, 'iops_min': 114098, 'iops_max': 235067, 'iops_mean': 174323.904167, 'iops_stddev': 10203.391614, 'iops_samples': 960}, 'write': {'io_bytes': 0, 'io_kbytes': 0, 'bw_bytes': 0, 'bw': 0, 'iops': 0.0, 'runtime': 0, 'total_ios': 0, 'short_ios': 0, 'drop_ios': 0, 'slat_ns': {'min': 0, 'max': 0, 'mean': 0.0, 'stddev': 0.0}, 'clat_ns': {'min': 0, 'max': 0, 'mean': 0.0, 'stddev': 0.0, 'percentile': {'1.000000': 0, '5.000000': 0, '10.000000': 0, '20.000000': 0, '30.000000': 0, '40.000000': 0, '50.000000': 0, '60.000000': 0, '70.000000': 0, '80.000000': 0, '90.000000': 0, '95.000000': 0, '99.000000': 0, '99.500000': 0, '99.900000': 0, '99.950000': 0, '99.990000': 0}}, 'lat_ns': {'min': 0, 'max': 0, 'mean': 0.0, 'stddev': 0.0}, 'bw_min': 0, 'bw_max': 0, 'bw_agg': 0.0, 'bw_mean': 0.0, 'bw_dev': 0.0, 'bw_samples': 0, 'iops_min': 0, 'iops_max': 0, 'iops_mean': 0.0, 'iops_stddev': 0.0, 'iops_samples': 0}, 'trim': {'io_bytes': 0, 'io_kbytes': 0, 'bw_bytes': 0, 'bw': 0, 'iops': 0.0, 'runtime': 0, 'total_ios': 0, 'short_ios': 0, 'drop_ios': 0, 'slat_ns': {'min': 0, 'max': 0, 'mean': 0.0, 'stddev': 0.0}, 'clat_ns': {'min': 0, 'max': 0, 'mean': 0.0, 'stddev': 0.0, 'percentile': {'1.000000': 0, '5.000000': 0, '10.000000': 0, '20.000000': 0, '30.000000': 0, '40.000000': 0, '50.000000': 0, '60.000000': 0, '70.000000': 0, '80.000000': 0, '90.000000': 0, '95.000000': 0, '99.000000': 0, '99.500000': 0, '99.900000': 0, '99.950000': 0, '99.990000': 0}}, 'lat_ns': {'min': 0, 'max': 0, 'mean': 0.0, 'stddev': 0.0}, 'bw_min': 0, 'bw_max': 0, 'bw_agg': 0.0, 'bw_mean': 0.0, 'bw_dev': 0.0, 'bw_samples': 0, 'iops_min': 0, 'iops_max': 0, 'iops_mean': 0.0, 'iops_stddev': 0.0, 'iops_samples': 0}, 'sync': {'lat_ns': {'min': 0, 'max': 0, 'mean': 0.0, 'stddev': 0.0, 'percentile': {'1.000000': 0, '5.000000': 0, '10.000000': 0, '20.000000': 0, '30.000000': 0, '40.000000': 0, '50.000000': 0, '60.000000': 0, '70.000000': 0, '80.000000': 0, '90.000000': 0, '95.000000': 0, '99.000000': 0, '99.500000': 0, '99.900000': 0, '99.950000': 0, '99.990000': 0}}, 'total_ios': 0}, 'job_runtime': 480036, 'usr_cpu': 4.060945, 'sys_cpu': 11.02084, 'ctx': 17152966, 'majf': 0, 'minf': 1072, 'iodepth_level': {'1': 0.1, '2': 0.1, '4': 0.1, '8': 0.1, '16': 0.1, '32': 0.1, '>=64': 99.998795}, 'iodepth_submit': {'0': 0.0, '4': 100.0, '8': 0.0, '16': 0.0, '32': 0.0, '64': 0.0, '>=64': 0.0}, 'iodepth_complete': {'0': 0.0, '4': 99.999981, '8': 0.0, '16': 0.0, '32': 0.0, '64': 0.0, '>=64': 0.1}, 'latency_ns': {'2': 0.0, '4': 0.0, '10': 0.0, '20': 0.0, '50': 0.0, '100': 0.0, '250': 0.0, '500': 0.0, '750': 0.0, '1000': 0.0}, 'latency_us': {'2': 0.0, '4': 0.0, '10': 0.0, '20': 0.0, '50': 0.0, '100': 0.01, '250': 0.01, '500': 0.145133, '750': 0.208033, '1000': 0.217177}, 'latency_ms': {'2': 0.945821, '4': 3.353353, '10': 91.070937, '20': 4.055254, '50': 0.0, '100': 0.0, '250': 0.0, '500': 0.0, '750': 0.0, '1000': 0.0, '2000': 0.0, '>=2000': 0.0}, 'latency_depth': 256, 'latency_target': 0, 'latency_percentile': 100.0, 'latency_window': 0}], 'disk_util': [{'name': 'nvme0n1', 'read_ios': 20907495, 'write_ios': 60, 'read_merges': 0, 'write_merges': 49, 'read_ticks': 122710597, 'write_ticks': 239, 'in_queue': 122710868, 'util': 99.967487}]}\n"
     ]
    }
   ],
   "source": [
    "job_outputs = []\n",
    "\n",
    "# Load the results\n",
    "for job_output in job_outputs_filename:\n",
    "    f = open(f\"{fio_jobs_path}/{job_output}\")\n",
    "    data = json.load(f)\n",
    "    job_outputs.append(data)\n",
    "\n",
    "print(job_outputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Test Longhorn - cluster IICT\n",
    "On souhaite déployer un container docker avec un PV dans le cluster IICT. Le container contient l'outil *fio* et un dossier *fio-jobs*.\n",
    "\n",
    "Architecting-it a fait des [scripts](https://github.com/architectingit/k8sstorage/blob/main/perfraw.sh) pour réaliser ses tests. On va s'inspirer de leur script pour réaliser nos tests.\n",
    "\n",
    "<u>**Connection au cluster IICT**</u>\n",
    "\n",
    "Connectez-vous au VPN de l'école:\n",
    "```bash\n",
    "$ kubectl config get-contexts         # is ` iict ` listed ?\n",
    "$ kubectl config use-context iict\n",
    "```\n",
    "\n",
    "<u>**Infrastructure testée**</u>\n",
    "\n",
    "TODO mail à Rémi"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Benchmarking manuel approfondi\n",
    "En plus de réaliser le benchmarking de la solution de stockage avec fio, on utilise les outils suivants pour monitorer le benchmarking :\n",
    "* iostat\n",
    "* top\n",
    "* iftop"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<u>**Exécution des tests**</u>\n",
    "\n",
    "Ensuite, on déploie un container :\n",
    "\n",
    "```bash\n",
    "$ kubectl apply -f deployments/deployment-longhorn-00-manual.yaml\n",
    "\n",
    "# monitoring tool 1\n",
    "$ kubectl exec --namespace=mercado -it $(kubectl get pods --namespace=mercado -o=jsonpath='{.items[0].metadata.name}') -- /bin/sh\n",
    "/ # iostat 10\n",
    "\n",
    "# monitoring tool 2\n",
    "$ kubectl exec --namespace=mercado -it $(kubectl get pods --namespace=mercado -o=jsonpath='{.items[0].metadata.name}') -- /bin/sh\n",
    "/ # iftop\n",
    "\n",
    "# monitoring tool 3\n",
    "$ kubectl exec --namespace=mercado -it $(kubectl get pods --namespace=mercado -o=jsonpath='{.items[0].metadata.name}') -- /bin/sh\n",
    "/ # top\n",
    "\n",
    "# benchmarking\n",
    "$ kubectl exec --namespace=mercado -it $(kubectl get pods --namespace=mercado -o=jsonpath='{.items[0].metadata.name}') -- /bin/sh\n",
    "/ # ./run-all-jobs.sh\n",
    "\n",
    "/ # exit # on sort des 4 sessions\n",
    "# get the benchmarking data\n",
    "$ ./iict-fio-benchmark-get-output.sh 00\n",
    "$ kubectl --delete deployments/deployment-longhorn-00-manual.yaml\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Image\n",
    "\n",
    "Nous allons déployer une image contenant les scripts de tests et un script d'exécution des tests en quelques lignes. On déploie :\n",
    "\n",
    "```bash\n",
    "$ kubectl apply -f iict-fio-benchmark.yaml\n",
    "```\n",
    "\n",
    "On peut observer l'exécution du container en consultant les logs (ou en allant sur l'UI rancher qui se rafraîchit automatiquement):\n",
    "```bash\n",
    "$ kubectl logs --follow $(kubectl get pods --namespace=mercado -o=jsonpath='{.items[0].metadata.name}') --namespace=mercado\n",
    "```\n",
    "\n",
    "Une fois que les logs affiche `\"All jobs done\"`, alors on peut récupérer les output et nettoyer les ressources:\n",
    "\n",
    "```bash\n",
    "$ sh iict-fio-benchmark-get-output.sh\n",
    "$ kubectl delete -f iict-fio-benchmark.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Maintenant on va visualiser nos données:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from: https://stackoverflow.com/a/3207973\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "fio_jobs_path = 'docker/iict/fio-jobs'\n",
    "job_outputs_filename = [f for f in listdir(fio_jobs_path) if isfile(join(fio_jobs_path, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fio version': 'fio-3.27', 'timestamp': 1625731118, 'timestamp_ms': 1625731118200, 'time': 'Thu Jul  8 07:58:38 2021', 'global options': {'name': 'read_bw', 'rw': 'randread', 'randrepeat': '0', 'verify': '0', 'ioengine': 'libaio', 'direct': '1', 'gtod_reduce': '1', 'bs': '128K', 'runtime': '30s', 'iodepth': '16', 'fdatasync': '0', 'size': '250G', 'ramp_time': '10s', 'filename': '/data/fiotest'}, 'jobs': [{'jobname': 'architecting-it-test3-read-bandwidth', 'groupid': 0, 'error': 0, 'eta': 0, 'elapsed': 41, 'read': {'io_bytes': 422051840, 'io_kbytes': 412160, 'bw_bytes': 13798405, 'bw': 13475, 'iops': 104.783078, 'runtime': 30587, 'total_ios': 3205, 'short_ios': 0, 'drop_ios': 0, 'slat_ns': {'min': 0, 'max': 0, 'mean': 0.0, 'stddev': 0.0, 'N': 0}, 'clat_ns': {'min': 0, 'max': 0, 'mean': 0.0, 'stddev': 0.0, 'N': 0}, 'lat_ns': {'min': 0, 'max': 0, 'mean': 0.0, 'stddev': 0.0, 'N': 0}, 'bw_min': 768, 'bw_max': 35398, 'bw_agg': 99.881225, 'bw_mean': 13459.918033, 'bw_dev': 6693.517365, 'bw_samples': 61, 'iops_min': 6, 'iops_max': 276, 'iops_mean': 105.081967, 'iops_stddev': 52.252686, 'iops_samples': 61}, 'write': {'io_bytes': 0, 'io_kbytes': 0, 'bw_bytes': 0, 'bw': 0, 'iops': 0.0, 'runtime': 0, 'total_ios': 0, 'short_ios': 0, 'drop_ios': 0, 'slat_ns': {'min': 0, 'max': 0, 'mean': 0.0, 'stddev': 0.0, 'N': 0}, 'clat_ns': {'min': 0, 'max': 0, 'mean': 0.0, 'stddev': 0.0, 'N': 0}, 'lat_ns': {'min': 0, 'max': 0, 'mean': 0.0, 'stddev': 0.0, 'N': 0}, 'bw_min': 0, 'bw_max': 0, 'bw_agg': 0.0, 'bw_mean': 0.0, 'bw_dev': 0.0, 'bw_samples': 0, 'iops_min': 0, 'iops_max': 0, 'iops_mean': 0.0, 'iops_stddev': 0.0, 'iops_samples': 0}, 'trim': {'io_bytes': 0, 'io_kbytes': 0, 'bw_bytes': 0, 'bw': 0, 'iops': 0.0, 'runtime': 0, 'total_ios': 0, 'short_ios': 0, 'drop_ios': 0, 'slat_ns': {'min': 0, 'max': 0, 'mean': 0.0, 'stddev': 0.0, 'N': 0}, 'clat_ns': {'min': 0, 'max': 0, 'mean': 0.0, 'stddev': 0.0, 'N': 0}, 'lat_ns': {'min': 0, 'max': 0, 'mean': 0.0, 'stddev': 0.0, 'N': 0}, 'bw_min': 0, 'bw_max': 0, 'bw_agg': 0.0, 'bw_mean': 0.0, 'bw_dev': 0.0, 'bw_samples': 0, 'iops_min': 0, 'iops_max': 0, 'iops_mean': 0.0, 'iops_stddev': 0.0, 'iops_samples': 0}, 'sync': {'total_ios': 0, 'lat_ns': {'min': 0, 'max': 0, 'mean': 0.0, 'stddev': 0.0, 'N': 0}}, 'job_runtime': 30586, 'usr_cpu': 0.228863, 'sys_cpu': 0.588505, 'ctx': 3206, 'majf': 0, 'minf': 58, 'iodepth_level': {'1': 0.0, '2': 0.0, '4': 0.0, '8': 0.0, '16': 100.0, '32': 0.0, '>=64': 0.0}, 'iodepth_submit': {'0': 0.0, '4': 100.0, '8': 0.0, '16': 0.0, '32': 0.0, '64': 0.0, '>=64': 0.0}, 'iodepth_complete': {'0': 0.0, '4': 99.968808, '8': 0.0, '16': 0.1, '32': 0.0, '64': 0.0, '>=64': 0.0}, 'latency_ns': {'2': 0.0, '4': 0.0, '10': 0.0, '20': 0.0, '50': 0.0, '100': 0.0, '250': 0.0, '500': 0.0, '750': 0.0, '1000': 0.0}, 'latency_us': {'2': 0.0, '4': 0.0, '10': 0.0, '20': 0.0, '50': 0.0, '100': 0.0, '250': 0.0, '500': 0.0, '750': 0.0, '1000': 0.0}, 'latency_ms': {'2': 0.0, '4': 0.0, '10': 0.0, '20': 0.0, '50': 0.0, '100': 0.0, '250': 0.0, '500': 0.0, '750': 0.0, '1000': 0.0, '2000': 0.0, '>=2000': 0.0}, 'latency_depth': 16, 'latency_target': 0, 'latency_percentile': 100.0, 'latency_window': 0}], 'disk_util': [{'name': 'sdc', 'read_ios': 3385, 'write_ios': 140, 'read_merges': 0, 'write_merges': 9, 'read_ticks': 646384, 'write_ticks': 57928, 'in_queue': 705404, 'util': 99.849698}]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "job_outputs = []\n",
    "\n",
    "# Load the results\n",
    "for job_output in job_outputs_filename:\n",
    "    f = open(f\"fio-jobs-output/iict/deployment-01-fs/{job_output}-output\")\n",
    "    data = json.load(f)\n",
    "    job_outputs.append(data)\n",
    "\n",
    "print(job_outputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Déploiement d'une nouvelle image\n",
    "\n",
    "Pour mettre à jour l'image que le cluster peut utiliser pour benchmark (avec de nouveaux scénarios de tests ou des modifications), on tag un commit de ce repository avec, par exemple :\n",
    "\n",
    "```bash\n",
    "$ git tag # what tag can we use?\n",
    "...\n",
    "v0.1.4\n",
    "v0.1.5\n",
    "v0.1.6\n",
    "v0.1.7\n",
    "\n",
    "$ git tag v0.1.8\n",
    "$ git push origin tag v0.1.8\n",
    "```\n",
    "\n",
    "On patiente un moment que l'image soit mise en ligne puis on peut réaliser notre benchmark.\n",
    "\n",
    "**Note**: La github Action ne build que les images en `v0.*.*` pour le moment."
   ]
  },
  {
   "source": [
    "# Traîtement de l'output\n",
    "En s'inspirant des 9 jobs fait par Architecting-IT, on va extraire les résultats:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fio-jobs-output/iict/deployment-01-fs\n"
     ]
    }
   ],
   "source": [
    "deployments_filename = [f.split('.')[0] for f in listdir('deployments/')]\n",
    "\n",
    "def deployment_output_folder_path(is_iict: bool, idx: int):\n",
    "    \"\"\"\n",
    "    :param is_iict: true if the deployment is related to IICT\n",
    "    :param idx: the deployment\n",
    "    :return: the deployment folder path\n",
    "    \"\"\"\n",
    "    folder = \"local\"\n",
    "    if is_iict:\n",
    "        folder = \"iict\"\n",
    "    dep = \"\"\n",
    "    for d in deployments_filename:\n",
    "        if str(idx) in d:\n",
    "            dep = d\n",
    "\n",
    "    return f\"fio-jobs-output/{folder}/{dep}\"\n",
    "\n",
    "print(deployment_output_folder_path(is_iict=True, idx=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'deployment name': 'fio-jobs-output/iict/deployment-01-fs', 'seq_read_bw': 339834.409753, 'seq_write_bw': 371325.31694, 'write_iops': 4127.314278, 'rw_mix_read': 149.973625, 'rw_mix_write': 51.002242, 'read_latency': 497229907.238867, 'read_iops': 1502.93177, 'read_bw_mean': 13459.918033, 'write_bw_mean': 3454.12, 'write_latency': 7082972.685346}\n"
     ]
    }
   ],
   "source": [
    "def deployment_output(deployment_output_folder: str):\n",
    "    \"\"\"\n",
    "    :param deployment_output_folder:\n",
    "    :return: a filtered view of all jobs with relevant metrics\n",
    "    \"\"\"\n",
    "    output = {}\n",
    "    output['deployment name'] = deployment_output_folder\n",
    "    for f in listdir(deployment_output_folder):\n",
    "        o = json.load(open(f\"{deployment_output_folder}/{f}\"))\n",
    "        if o['global options']['name'] == 'read_iops':\n",
    "            # iops_mean is a stop criteria?\n",
    "            output['read_iops'] = o['jobs'][0]['read']['iops']\n",
    "        if o['global options']['name'] == 'write_iops':\n",
    "            output['write_iops'] = o['jobs'][0]['write']['iops']\n",
    "        if o['global options']['name'] == 'read_bw':\n",
    "            output['read_bw_mean'] = o['jobs'][0]['read']['bw_mean']\n",
    "        if o['global options']['name'] == 'write_bw':\n",
    "            output['write_bw_mean'] = o['jobs'][0]['write']['bw_mean']\n",
    "        # TODO rerun jobs with gtod_reduce=0\n",
    "        if o['global options']['name'] == 'read_latency':\n",
    "            # slat: submission to kernel latency\n",
    "            # clat: submission+completion latency\n",
    "            # lat: TODO verify ~s+clat\n",
    "            output['read_latency'] = o['jobs'][0]['read']['lat_ns']['mean']\n",
    "        if o['global options']['name'] == 'write_latency':\n",
    "             output['write_latency'] = o['jobs'][0]['write']['lat_ns']['mean']\n",
    "        if o['global options']['name'] == 'seq_read':\n",
    "            output['seq_read_bw'] = o['jobs'][0]['read']['bw_mean']\n",
    "        if o['global options']['name'] == 'seq_write':\n",
    "            output['seq_write_bw'] = o['jobs'][0]['write']['bw_mean']\n",
    "        if o['global options']['name'] == 'rw_mix':\n",
    "            output['rw_mix_read'] = o['jobs'][0]['read']['iops']\n",
    "            output['rw_mix_write'] = o['jobs'][0]['write']['iops']\n",
    "    return output\n",
    "\n",
    "deployment_01 = deployment_output(deployment_output_folder_path(True, 1))\n",
    "print(deployment_01)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Graphiques\n",
    "On veut réaliser une comparaison avec des graphiques sur les différentes métriques testées."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: 'fio-jobs-output/iict//deployment-07-network-effect-remote-replica'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIsADirectoryError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-7-4dc08a91fccf>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      7\u001B[0m     \u001B[0mdeployment_output\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdeployment_output_folder_path\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m6\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m     \u001B[0mdeployment_output\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdeployment_output_folder_path\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m7\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 9\u001B[0;31m     \u001B[0mdeployment_output\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdeployment_output_folder_path\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m8\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     10\u001B[0m     \u001B[0mdeployment_output\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdeployment_output_folder_path\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m9\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m ]\n",
      "\u001B[0;32m<ipython-input-6-46719106b2b6>\u001B[0m in \u001B[0;36mdeployment_output\u001B[0;34m(deployment_output_folder)\u001B[0m\n\u001B[1;32m      7\u001B[0m     \u001B[0moutput\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'deployment name'\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdeployment_output_folder\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mf\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mlistdir\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdeployment_output_folder\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 9\u001B[0;31m         \u001B[0mo\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mjson\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"{deployment_output_folder}/{f}\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     10\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mo\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'global options'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'name'\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m'read_iops'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m             \u001B[0;31m# iops_mean is a stop criteria?\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mIsADirectoryError\u001B[0m: [Errno 21] Is a directory: 'fio-jobs-output/iict//deployment-07-network-effect-remote-replica'"
     ]
    }
   ],
   "source": [
    "deployments = [\n",
    "    deployment_output(deployment_output_folder_path(True, 1)),\n",
    "    deployment_output(deployment_output_folder_path(True, 2)),\n",
    "    deployment_output(deployment_output_folder_path(True, 3)),\n",
    "    deployment_output(deployment_output_folder_path(True, 4)),\n",
    "    deployment_output(deployment_output_folder_path(True, 5)),\n",
    "    deployment_output(deployment_output_folder_path(True, 6)),\n",
    "    deployment_output(deployment_output_folder_path(True, 7)),\n",
    "    deployment_output(deployment_output_folder_path(True, 8)),\n",
    "    deployment_output(deployment_output_folder_path(True, 9)),\n",
    "]\n",
    "\n",
    "deployments_name = [ d['deployment name'].split('0')[1] for d in deployments]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# default figure file type and dpi\n",
    "figure_format = 'svg'\n",
    "figure_dpi = 1200"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.ioff()\n",
    "from IPython.display import SVG, display\n",
    "\n",
    "read_iops = [d['read_iops'] for d in deployments]\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "# display legends: https://stackoverflow.com/a/19576608\n",
    "ax = fig.add_axes([0.1,0.1,0.75,0.75])\n",
    "ax.set_title('Read IOPS')\n",
    "ax.set_xlabel('deployments')\n",
    "ax.set_ylabel('IOPS')\n",
    "bar = plt.bar(deployments_name, read_iops)\n",
    "img_name = 'figures/read_iops.' + figure_format\n",
    "plt.savefig(img_name, format=figure_format, dpi=figure_dpi)\n",
    "display(SVG(img_name))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "write_iops = [d['write_iops'] for d in deployments]\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "ax = fig.add_axes([0.1,0.1,0.75,0.75])\n",
    "ax.set_title('Write IOPS')\n",
    "ax.set_xlabel('deployments')\n",
    "ax.set_ylabel('IOPS')\n",
    "bar = plt.bar(deployments_name, write_iops)\n",
    "img_name = 'figures/write_iops.' + figure_format\n",
    "plt.savefig(img_name, format=figure_format, dpi=figure_dpi)\n",
    "display(SVG(img_name))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "read_bw_means = [d['read_bw_mean'] for d in deployments]\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "ax = fig.add_axes([0.1,0.1,0.75,0.75])\n",
    "ax.set_title('Mean read bandwidth')\n",
    "ax.set_xlabel('deployments')\n",
    "ax.set_ylabel('Bandwidth [???]') # TODO unit\n",
    "bar = plt.bar(deployments_name, read_bw_means)\n",
    "img_name = 'figures/read_bw_means.' + figure_format\n",
    "plt.savefig(img_name, format=figure_format, dpi=figure_dpi)\n",
    "display(SVG(img_name))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "write_bw_means = [d['write_bw_mean'] for d in deployments]\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "ax = fig.add_axes([0.1,0.1,0.75,0.75])\n",
    "ax.set_title('Mean write bandwidth')\n",
    "ax.set_xlabel('deployments')\n",
    "ax.set_ylabel('Bandwidth [???]') # TODO unit\n",
    "bar = plt.bar(deployments_name, write_bw_means)\n",
    "img_name = 'figures/write_bw_means.' + figure_format\n",
    "plt.savefig(img_name, format=figure_format, dpi=figure_dpi)\n",
    "display(SVG(img_name))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "read_lats = [d['read_latency'] for d in deployments]\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "ax = fig.add_axes([0.1,0.1,0.75,0.75])\n",
    "ax.set_title('Mean read latency')\n",
    "ax.set_xlabel('deployments')\n",
    "ax.set_ylabel('Latency [???]') # TODO unit\n",
    "bar = plt.bar(deployments_name, read_lats)\n",
    "img_name = 'figures/read_latency_means.' + figure_format\n",
    "plt.savefig(img_name, format=figure_format, dpi=figure_dpi)\n",
    "display(SVG(img_name))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "write_lats = [d['write_latency'] for d in deployments]\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "ax = fig.add_axes([0.1,0.1,0.75,0.75])\n",
    "ax.set_title('Mean write latency')\n",
    "ax.set_xlabel('deployments')\n",
    "ax.set_ylabel('Latency [???]') # TODO unit\n",
    "bar = plt.bar(deployments_name, write_lats)\n",
    "img_name = 'figures/write_latency_means.' + figure_format\n",
    "plt.savefig(img_name, format=figure_format, dpi=figure_dpi)\n",
    "display(SVG(img_name))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO seq\n",
    "seq_read_bws = [d['seq_read_bw'] for d in deployments]\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "ax = fig.add_axes([0.1,0.1,0.75,0.75])\n",
    "ax.set_title('Mean sequential read bandwidth')\n",
    "ax.set_xlabel('deployments')\n",
    "ax.set_ylabel('Bandwidth [???]') # TODO unit\n",
    "bar = plt.bar(deployments_name, seq_read_bws)\n",
    "img_name = 'figures/seq_read_bandwidth_means.' + figure_format\n",
    "plt.savefig(img_name, format=figure_format, dpi=figure_dpi)\n",
    "display(SVG(img_name))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seq_write_bws = [d['seq_write_bw'] for d in deployments]\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "ax = fig.add_axes([0.1,0.1,0.75,0.75])\n",
    "ax.set_title('Mean sequential write bandwidth')\n",
    "ax.set_xlabel('deployments')\n",
    "ax.set_ylabel('Bandwidth [???]') # TODO unit\n",
    "bar = plt.bar(deployments_name, seq_read_bws)\n",
    "img_name = 'figures/seq_write_bandwidth_means.' + figure_format\n",
    "plt.savefig(img_name, format=figure_format, dpi=figure_dpi)\n",
    "display(SVG(img_name))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# TODO grouped bw\n",
    "# https://matplotlib.org/stable/gallery/lines_bars_and_markers/barchart.html\n",
    "x = np.arange(len(deployments))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10.5, 5.5) # manually adjust for names to fit\n",
    "rects1 = ax.bar(x - width/2, read_bw_means, width=width, label = 'random read')\n",
    "rects2 = ax.bar(x + width/2, write_bw_means, width=width, label = 'random write')\n",
    "\n",
    "ax.set_ylabel('Bandwidth [???]') # TODO unit\n",
    "ax.set_title('Bandwidth comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(deployments_name)\n",
    "ax.legend()\n",
    "\n",
    "ax.bar_label(rects1, padding=3)\n",
    "ax.bar_label(rects2, padding=3)\n",
    "\n",
    "# fig.tight_layout()\n",
    "\n",
    "img_name = 'figures/bandwidth_comparison.' + figure_format\n",
    "plt.savefig(img_name, format=figure_format, dpi=figure_dpi)\n",
    "display(SVG(img_name))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Configuration de Longhorn testée\n",
    "* replicas\n",
    "* [data locality](https://longhorn.io/docs/1.1.0/high-availability/data-locality/#data-locality-settings) (replica sur le même node que le workload)\n",
    "* scheduling policy?\n",
    "* volumeMode\n",
    "\n",
    "[I/O operations and replicas](https://longhorn.io/docs/1.1.1/concepts/#231-how-read-and-write-operations-work-for-replicas)\n",
    "\n",
    "Dans un volume Longhorn, lorsque les opérations I/O sont faites sur une replica, ces opérations se font d'abord sur le live data. Si le bloc cherché n'est pas sur le live data, alors, il va cherché le bloc dans la snapshot la plus récente jusqu'à ce qu'il trouve la bonne snapshot. Une fois trouvé, un index est conservé pour se souvenir de la snapshot la plus récente contenant le bloc recherché.\n",
    "\n",
    "L'index est conservé en mémoire et a une taille de 1 byte par block de taille 4K.\n",
    "\n",
    "[taille effective des volumes/replicas/snapshot](https://longhorn.io/docs/1.1.1/volumes-and-nodes/volume-size/#an-example-that-helps-understand-volume-size-and-volume-actual-size)\n",
    "\n",
    "Au départ, les volumes ne prennent pas de places. Mais après qu'un document soit supprimé du FS dans le volume, cela n'est pas répliqué au niveau bloc: *\"The truth of the data#1 deletion is that the data#1 is marked as removed in the filesystem level (For example, inode deletion in ext4). Since Longhorn operates on the block level and does not understand the filesystem, as a result, the disk blocks/space storing data#1 won’t be released after the deletion.\"*\n",
    "\n",
    "Est-ce que ce serait intéressant de démontrer ça?\n",
    "\n",
    "[Longhorn engine par volume](https://longhorn.io/docs/1.1.1/concepts/#11-the-longhorn-manager-and-the-longhorn-engine)\n",
    "\n",
    "Super intéressant.\n",
    "\n",
    "[replica par défaut](https://longhorn.io/docs/1.1.1/references/settings/#default-replica-count)\n",
    "\n",
    "3 apparament??? Est-ce que je peux le vérifier moi-même?\n",
    "\n",
    "[volumeMode block](https://longhorn.io/docs/0.8.0/examples/block-volume/)\n",
    "\n",
    "what is this\n",
    "\n",
    "[PV avec csi option, numberofreplicas](https://longhorn.io/docs/0.8.0/examples/csi-pv/)\n",
    "\n",
    "[volumeMode: Filesystem](https://github.com/longhorn/longhorn/blob/8318d99136e0d5b39f01a37adf9cae5342034d7e/examples/csi/example_pv.yaml#L8)\n",
    "\n",
    "[persistentVolumeReclaimPolicy: Delete](https://github.com/longhorn/longhorn/blob/8318d99136e0d5b39f01a37adf9cae5342034d7e/examples/csi/example_pv.yaml#L11)\n",
    "\n",
    "\n",
    "Note: [google](https://cloud.google.com/compute/docs/disks/benchmarking-pd-performance) persistent disk benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Nombre de replicas\n",
    "\n",
    "Par défaut, le nombre de replicas est de 3 tel que montré:\n",
    "\n",
    "![idk](img/default-pvc-replicas.png)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "source": [
    "On va tester les performances lorsqu'il y a 0, 1, 3 (par défaut) et 5 replicas (deployment-03-06).\n",
    "\n",
    "<u>test 7 pbs</u>\n",
    "* erreur 1 ligne (0 replica): il y a eu une erreur au test7 qui ne s'est pas reproduit. C'est étrange.\n",
    "* erreurs multiples (1 replica): fio: io_u error on file /data/fiotest: Read-only file system: read offset=1048576000, buflen=1048576\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t = deployment_output(deployment_output_folder_path(True, 3))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "source": [
    "## Local/remote volume with/without remote replica\n",
    "\n",
    "On observer l'efficacité d'un *deployment* avec le *workload* et le volume dans un même *node*, puis sur deux *nodes*. On veut aussi voir comment une *replica* du *volume* affecte les performances lorsque la *replica* se trouve sur le même *node* que le *volume* et lorsque la *replica* est sur un *node* différent du *volume* associé.\n",
    "\n",
    "On utilise l'attribut *node-selector* pour placer le *workload* sur un *node*. Les *nodes* disponibles sur le cluster sont:\n",
    "```bash\n",
    "$ kubectl get node --show-labels\n",
    "NAME           STATUS   ROLES                      AGE   VERSION   LABELS\n",
    "10.193.72.32   Ready    controlplane,etcd,worker   45h   v1.19.3   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=10.193.72.32,kubernetes.io/os=linux,node-role.kubernetes.io/controlplane=true,node-role.kubernetes.io/etcd=true,node-role.kubernetes.io/worker=true\n",
    "10.193.72.33   Ready    worker                     45h   v1.19.3   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=10.193.72.33,kubernetes.io/os=linux,node-role.kubernetes.io/worker=true\n",
    "10.193.72.34   Ready    worker                     45h   v1.19.3   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=10.193.72.34,kubernetes.io/os=linux,node-role.kubernetes.io/worker=true\n",
    "\n",
    "```\n",
    "\n",
    "Chaque node possède un label permettant de les distinguer : `kubernetes.io/hostname`. Ce sera l'argument utilisé dans `node-selector`.\n",
    "\n",
    "Lorsque l'on veut que le *workload* puisse communiquer avec le *volume* en étant placé sur un *node* différent, on va désactiver le paramètre de [`data locality`](https://longhorn.io/docs/1.1.2/high-availability/data-locality/#data-locality-settings) du *volume*. Dans un des tests, on va laisser `data-locality` à `best-effort` pour garder une *replica* sur le même node que le *workload* et le *volume*.\n",
    "\n",
    "Contrairement à Architecting-IT, une *StorageClass* ne sera pas utilisé. Chaque volume est créé manuellement depuis le `Longhorn UI` avant d'appliquer le *manifest*. Cela permet d'attacher le volume à un *node*.\n",
    "\n",
    "### Replica zone level soft anti-affinity\n",
    "Afin de constater\n",
    "* l'IOPS au maximum (deployment 06)\n",
    "* l'effet de séparer le *workload* et le *volume* (deployment 08)\n",
    "\n",
    "on désire minimiser les effets du networking (`...no-replica`). Pour cela, on active le paramètre [`Replica Zone Level Soft Anti-Affinity`](https://longhorn.io/docs/1.1.2/best-practices/#replica-node-level-soft-anti-affinity) pour autoriser qu'une replica soit sur le même *node* que le *volume* associé. Cependant cela ne donne aucune guarantie que la replica sera séparée du workload.\n",
    "\n",
    "**Note**: que c'est une mauvaise idée de d'activer [`Replica Zone Level Soft Anti-Affinity`](https://longhorn.io/docs/1.1.2/best-practices/#replica-node-level-soft-anti-affinity) en production parce que si le *node* tombe, alors la *replica* aussi.\n",
    "\n",
    "### Test\n",
    "Pour benchmark les *deployments* 6-9, on crée un volume depuis le [Longhorn UI](https://kubernetes.iict.ch/dashboard/c/c-f9654/longhorn) (cliquer sur Longhorn pour être amener à l'UI), dans l'onglet `Volume`, avec les paramètres suivants:\n",
    "\n",
    "deployment | 06-local-volume-no-replica | 07-local-volume-with-replica | 08-remote-volume-no-replica | 09-remote-volume-with-replica\n",
    "---- | --- | --- |--- |---\n",
    "Nom volume | fiobench-vol| fiobench-vol| fiobench-vol| fiobench-vol\n",
    "Size | 1000Gi| 1000Gi| 1000Gi| 1000Gi\n",
    "Number of replicas | 1| 1| 1| 1\n",
    "Frontend | Block device| Block device| Block device| Block device\n",
    "Data locality | best effort | disabled| disabled| disabled\n",
    "Access mode | ReadWriteOnce| ReadWriteOnce| ReadWriteOnce| ReadWriteOnce\n",
    "nodeSelector | node1 | node1 | node2 | node2\n",
    "Replica Node Level Soft Anti-Affinity (onglet `settings > general` (ne pas oublier de sauver))  | ☑️ | ☐ |☑️ | ☐\n",
    "\n",
    "Puis on applique le manifest `deployment-XX` associé.\n",
    "\n",
    "Vous pouvez vérifier que le volume et sa replica sont sur des *nodes* différents en inspectant les détails du volume dans l'UI Longhorn:\n",
    "\n",
    "![Volume séparé de la replica](img/vol-node-separated.png)\n",
    "\n",
    "FIXME pourquoi on ne peut pas attacher le volume à un node et nodeSelector le workload à un autre ? Pour le moment, je peux faire des storageClass avec les nodesSelector mais ça ne déplace pas le volume sur le bon node.\n",
    "\n",
    "* nodeSelector sur une sc, effet (pas le vol, pas replica, longhorn engine??)?\n",
    "* attach PV mais nodeSelector sur workload empêche (failed to mount) TODO example\n",
    "* split replica du volume n'est pas guaranti avec 1 replica et soft node anti-affinity\n",
    "* est-ce qu'il ne faudrait pas 2 replicas avec node soft anti-affinity disabled pour assurer des replicas séparées? When this setting is un-checked, the Longhorn Manager will not allow scheduling on nodes with existing healthy replicas of the same volume. [lien](https://longhorn.io/docs/1.1.2/references/settings/#replica-node-level-soft-anti-affinity)\n",
    "\n",
    "Note: test 8 config 8 a fail une fois, fio not the best\n",
    "\n",
    "[instance manager](https://longhorn.io/docs/1.1.2/terminology/#instance-manager) va bouger sur le `nodeSelector`???\n",
    "\n",
    "## Test cloud publique : AWS\n",
    "On désire comparer le cluster IICT et un service publique. On prend ici le\n",
    "\n",
    "### EC2\n",
    "Parmi les offres d'instances, une portion maximise l'IOPS des volumes EBS: *EBS-optimized instances*. Par exemple:\n",
    "* i3, very optimized, ssd\n",
    "* i3en, optimized, ssd (i3 > i3en)\n",
    "* le reste est du hdd, pas super pour les IOPS, surtout avec le random access\n",
    "\n",
    "Après discussion avec Marcel Graf, on penche pour une solution générale car on ne cherche pas à savoir ce qui se fait de mieux. Parmi les [instances](https://aws.amazon.com/ec2/instance-types/) existantes, on note :\n",
    "\n",
    "* T3, l'option low-cost, non EBS-optimized\n",
    "* T4g, EBS-optimized (pas recherchée)\n",
    "* M5, avec des meilleures performances réseau que T3 mais aucune mention que ce soit bon marché\n",
    "* A1, avec une penchant pour l'écosystème ARM\n",
    "\n",
    "On retient t3.small.\n",
    "\n",
    "D'après les [limitations](https://awseducate-starter-account-services.s3.amazonaws.com/AWS_Educate_Starter_Account_Services_Supported.pdf) du starter account:\n",
    "1. Instances limitées : *\"Starter Accounts supports only following instance types:“t2.small”, “t2.micro”, “t2.medium”, “c5.large”, “m5.large”\"*\n",
    "2. Ne peut pas créer d'utilisateur IAM : *\"You can create users, but cannot associate login profile or access keys for them\"*\n",
    "\n",
    "Bonjou Graf Marcel ,\n",
    "avec le compte aws starter account, il y a beaucoup de restrictions quand à la création de cluster, création de user IAM. Je ne peux pas élever mes privilèges suffisamment pour créer un cluster via eksctl. Comme workarount, je peux utiliser AWS console pour créer une instance EC2 avec un volume EBS et probablement ssh dans l'instance pour monter le volume, télécharger mes scripts...\n",
    "Mais ce n'est pas un cluster kubernetes.\n",
    "\n",
    "### EBS\n",
    "Je ne devrais pas trop avoir peur de la taille des données que je génère : *\"For example, let’s say that you provision a 2000 GB volume for 12 hours (43,200 seconds) in a 30-day month. In a region that charges $\\$$0.08 per GB-month, you would be charged $\\$$2.667 for the volume ($\\$$0.08 per GB-month * 2000 GB * 43,200 seconds / (86,400 seconds/day * 30 day-month)).\"*\n",
    "\n",
    "Je veux ~750GB pour 30 minutes. Ca devrait aller si je supprime le volume après directement.\n",
    "\n",
    "Même chose pour les IOPS: *\"For example, let’s say that you provision a 2000 GB volume for 12 hours (43,200 seconds) in a 30-day month. In a region that charges $\\$$0.08 per GB-month, you would be charged $\\$$2.667 for the volume ($\\$$0.08 per GB-month * 2000 GB * 43,200 seconds / (86,400 seconds/day * 30 day-month)).\"*\n",
    "\n",
    "**Note**: le job 1 génère ~45'000 total IO avec le deployement 1 IICT\n",
    "\n",
    "Alors je ne devrais pas trop m'inquiéter, d'après leur calculateur de prix:<br>\n",
    "    1 instances x 1 instance hours = 1.00 total instance hours<br>\n",
    "    1.00 instance hours / 730 hours in a month = 0.00 instance months<br>\n",
    "    EBS Storage Cost: 0 USD<br>\n",
    "    16,000 iops - 3000 GP3 iops free = 13,000.00 billable gp3 iops<br>\n",
    "    Max (13000.00 iops, 0 minimum billable iops) = 13,000.00 total billable gp3 iops<br>\n",
    "    1,000 MBps - 125 GP3 MBps free = 875.00 billable MBps<br>\n",
    "    Max (875.00 MBps, 0 minimum mbps) = 875.00 billable throughput (MBps)<br>\n",
    "    875.00 MBps / 1024 MB per GB = 0.85 billable throughput (GBps)<br>\n",
    "    Total snapshots: 30<br>\n",
    "    Initial snapshot cost: 1000 GB x 0.0500000000 = 50 USD<br>\n",
    "    Monthly cost of each snapshot: 3 GB x 0.0500000000 USD = 0.15 USD<br>\n",
    "    Discount for partial storage month: 0.15 USD x 50% = 0.075 USD<br>\n",
    "    Incremental snapshot cost: 0.075 USD x 30 = 2.25 USD<br>\n",
    "    Total snapshot cost: 50 USD + 2.25 USD = 52.25 USD<br>\n",
    "\n",
    "    Amazon Elastic Block Storage (EBS) total cost (monthly): 0.00 USD\n",
    "\n",
    "**Note**: il existe un [guide](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSPerformance.html) pour optimiser les performances des volumes EBS.\n",
    "\n",
    "### EBS-volumes\n",
    "Un volume est par défaut répliqué un nombre inconnu de fois dans la même *Availability Zone*. Tous les volumes EBS ont au moins 99.8-99.9 % de durabilité ([source](https://aws.amazon.com/ebs/features/#Amazon_EBS_availability_and_durability))\n",
    "\n",
    "Dans la définition de durabilité d'AWS, on comprends qu'il y au minimum deux replicas qui préviendrait la perte d'un centre d'activités ([source](https://docs.aws.amazon.com/whitepapers/latest/aws-storage-services-overview/durability-and-availability-3.html)):\n",
    "*\"EBS volume data is replicated across multiple servers in a single Availability Zone to prevent the loss of data from the failure of any single component.\"*\n",
    "\n",
    "La [doc](https://docs.aws.amazon.com/whitepapers/latest/aws-storage-services-overview/durability-and-availability-3.html) conseille de créer des snapshots pour prévenir le cas où une *Availability Zone* tombe: *\"Because an EBS volume is created in a particular Availability Zone, the volume will be unavailable if the Availability Zone itself is unavailable. A snapshot of a volume, however, is available across all of the Availability Zones within a Region, and you can use a snapshot to create one or more new EBS volumes in any Availability Zone in the region.\"*\n",
    "\n",
    "Puisque toute la technologie Amazon est *closed source*, on en saura pas plus que ce qu'il nous dise sans autre guarantie que leur parole.\n",
    "\n",
    "### Mise en place\n",
    "Avant de créer un cluster on a besoin de :\n",
    "* une policy\n",
    "* un rôle\n",
    "* un utilisateur\n",
    "\n",
    "#### Policy\n",
    "Aller sous l'onglet JSON et copier-coller le snippet suivant :\n",
    "```json\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"eks:*\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"iam:PassRole\",\n",
    "            \"Resource\": \"*\",\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\n",
    "                    \"iam:PassedToService\": \"eks.amazonaws.com\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "Passer l'écran d'ajout de tags.\n",
    "\n",
    "Nommez la policy `fiobenchAdminPolicy`\n",
    "\n",
    "#### Rôle\n",
    "Sous l'onglet de l'utilisateur, on récupère l'identifiant (les 12 chiffres) que l'on copie-colle dans le champ `Account ID`:\n",
    "![](img/aws-role-creation.png)\n",
    "\n",
    "Filtrer 'fiobenchAdminPolicy', sélectionnez la policy pour l'attacher au rôle et passez au suivant deux fois.\n",
    "\n",
    "Donnez le nom 'eks-admin' au rôle puis finaliser le rôle.\n",
    "\n",
    "#### Utilisateur\n",
    "On crée un utilisateur avec les paramètres suivants:\n",
    "- User name: fiobench-admin\n",
    "- [x] Programmatic access\n",
    "- [x] AWS Management Console access\n",
    "\n",
    "![](img/aws-user-creation.png)\n",
    "\n",
    "**Note**: dans un système RBAC, on crée un groupe associé à un rôle que l'utilisateur rejoint mais on va au plus simple\n",
    "\n",
    "Attachez la policy 'fiobenchAdminPolicy' à l'utilisateur, passez au suivant deux fois puis finalisez l'utilisateur.\n",
    "![](img/aws-user-creation-2.png)\n",
    "\n",
    "TODO il me manque les IAM permissions pour créer un utilisateur... Comment est-ce que je peux contourner ça?\n",
    "     User: arn:aws:sts::798210458137:assumed-role/vocstartsoft/user1368968=pablo.mercado@heig-vd.ch is not authorized to perform: iam:CreateAccessKey on resource: user admin with an explicit deny\n",
    "\n",
    "je peux seulement lancer une instance dans le free tier: t2.micro\n",
    "\n",
    "\n",
    "```bash\n",
    "sudo apt install awscli\n",
    "aws iam create-role --role-name fiobench --assume-role-policy-document file://iam-policy.json\n",
    "kubectl auth can-i \"*\" \"*\" # if well configured, the answer should be \"yes\"\n",
    "```\n",
    "\n",
    "On stocke l'output dans le fichier `role.json`.\n",
    "\n",
    "Le cluster de l'école utilise la v1.19.3\n",
    "Kubernetes Version.\n",
    "\n",
    "```bash\n",
    "eksctl create cluster \\\n",
    " --name fiobench \\\n",
    " --version 1.19 \\\n",
    " --nodegroup-name fiobench-node \\\n",
    " --node-type t3.small \\\n",
    " --nodes 1\n",
    "aws eks update-kubeconfig --name fiobench\n",
    "```\n",
    "\n",
    "\n",
    "imma follow [this](https://youtu.be/p6xDCz00TxU)\n",
    "\n",
    "En regardant le type d'instance, je pense que aim pour [i3.xlarge](https://aws.amazon.com/ec2/instance-types/i3/) peut le faire:\n",
    "* ~1TB de ssd\n",
    "* pour les workload requièrant beaucoup d'IOPS: optimized for low latency, very high random I/O performance, and high sequential read throughput, and deliver high IOPS at a low cost.\n",
    "\n",
    "Je dois m'assurer que je peux :\n",
    "* utiliser mon image\n",
    "* ssh\n",
    "* monter un volume\n",
    "* run mes scripts\n",
    "\n",
    "```bash\n",
    "eksctl delete cluster --name fiobench-cluster\n",
    "```\n",
    "\n",
    "ec2 instance [comparison](https://instances.vantage.sh/)\n",
    "\n",
    "wait, est-ce que je veux de l'instance storage ou un volume persistant? Ouaip, je veux un volume persistent... D'un côté j'ai une instance optimisée pour les IOPS mais peu modulable et de l'autre j'ai qqch de modulaire mais non optimisé qui risque de prendre plus de temps à déployer. Ouais, je n'ai pas nécessairement besoin d'une instance optimisée pour autre chose. On fait confiance à Amazon pour le stockage block.\n",
    "\n",
    "Je veux que mon utilisateur ait les [permissions](https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi.html) nécessaires pour déployer un volume etc...\n",
    "\n",
    "Deploy a sample Linux workload, book chapter\n",
    "\n",
    "configure aws [configmap](https://docs.aws.amazon.com/eks/latest/userguide/add-user-role.html)\n",
    "\n",
    "This is so complicated wtf, it's so easy to get lost. I guess Amazon [eks](https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html) is what I want?\n",
    "\n",
    "Vocareum semble à la base ne pas laisser configurer des utilisateurs à la main: https://www.reddit.com/r/aws/comments/llb5ff/aws_educate_account_doesnt_have_permission_to_do/gqgqr21/\n",
    "\n",
    "Can't access root account [link](https://stackoverflow.com/a/62965656)\n",
    "\n",
    "Je give up de configurer à la main :\n",
    "TODO last resort\n",
    "\n",
    "### Yolo\n",
    "```bash\n",
    "> aws configure\n",
    "...\n",
    "Default region name [None]: us-west-1\n",
    "> aws ec2 create-key-pair --key-name my-key-pair --query \"KeyMaterial\" --output text > my-key-pair.pem\n",
    "> chmod 400 my-key-pair.pem\n",
    "> eksctl create cluster \\\n",
    " --name fiobench-mercado \\\n",
    " --region us-west-1 \\\n",
    " --version 1.19 \\\n",
    " --nodegroup-name fiobench-node \\\n",
    " --node-type t3.small \\\n",
    " --nodes 1 \\\n",
    " --managed\n",
    "\n",
    " ...\n",
    " 2021-07-16 14:35:53 [✔]  EKS cluster \"fiobench-mercado\" in \"us-west-1\" region is ready\n",
    "\n",
    "# dans une autre console\n",
    "> aws eks --region us-west-1 update-kubeconfig --name fiobench-mercado\n",
    "Added new context arn:aws:eks:us-west-1:396229223364:cluster/fiobench-mercado to /home/brassens/.kube/config\n",
    "> kubectl auth can-i \"*\" \"*\" # can you perform anything?\n",
    "yes\n",
    "> kubectl apply -f deployments/aws.yaml\n",
    "> kubectl get pod # verify pod is doing something, when it is running, do:\n",
    "> kubectl logs --follow $(kubectl get pods -o=jsonpath='{.items[0].metadata.name}')\n",
    "# If you feel going to see what's inside\n",
    "> kubectl exec -it $(kubectl get pods -o=jsonpath='{.items[0].metadata.name}') -- /bin/sh\n",
    "/ # top\n",
    "...\n",
    "CPU:   1% usr   2% sys   0% nic   3% idle  90% io   0% irq   0% sirq\n",
    "# we see io number is high, indicating high activity\n",
    "...\n",
    "> ./aws-fio-benchmark-get-output.sh\n",
    "> kubectl delete -f deployments/aws.yaml\n",
    "> eksctl delete cluster --region=us-west-1 --name=fiobench-mercado\n",
    "```\n",
    "\n",
    "**Note**: Il semble que la suite de tests fio est trop demandeur ? J'ai interrompu le test 7 avec `ps kill`, les tests 8 et 9 se sont fait (suite du script). Ensuite, j'ai exécuté le test 7 à la main, puis j'ai récupéré les données.\n"
   ],
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "interpreter": {
   "hash": "72cb84ee8889e4f5449c4efd23c39e13f10da0b153d3918057aa0634ac1d69f8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
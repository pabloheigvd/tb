# how to demonstrate:
# créer un pv (1000Gi, fiobench-vol, 1 replica, node tag: node2)

# Events:
#  Type     Reason            Age   From               Message
#  ----     ------            ----  ----               -------
#  Warning  FailedScheduling  9s    default-scheduler  0/3 nodes are available: 1 node(s) had volume node affinity
  # conflict, 2 node(s) didn't match node selector.
#  Warning  FailedScheduling  9s    default-scheduler  0/3 nodes are available: 1 node(s) had volume node affinity
  # conflict, 2 node(s) didn't match node selector.

# En sachant que malgrès que la policy dataLocality modifiée, on lit:
# When data locality is disabled, a Longhorn volume can be backed by replicas on any nodes in the cluster and
  # accessed by a pod running on any node in the cluster.
# https://longhorn.io/docs/1.1.2/high-availability/data-locality/

# Je ne pense pas avoir fait une erreur dans ce deployment. Je pense que c'est une erreur dans la documentation. Les
# tests d'Architecting-IT ont peut-être été fait sur une version plus ancienne de Longhorn avec des features différentes
# mais franchement j'en doute. Je ne vois pas comment il a pu possiblement réaliser les deux tests où le workload et
# le volume se trouvent sur des nodes différents.
apiVersion: v1
kind: PersistentVolume
metadata:
  name: fiobench-pv
  namespace: mercado
  labels:
    fiobench: image
    deployment: test
    subject: separated-workload-from-volume
spec:
  storageClassName: longhorn-static
  capacity:
    storage: 1000Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  csi:
    driver: driver.longhorn.io
    fsType: ext4
    volumeAttributes:
      numberOfReplicas: '1'
      staleReplicaTimeout: '2880'
      dataLocality: 'disabled'
      nodeSelector: 'node2'
    volumeHandle: fiobench-vol # refers to volume on node
  nodeAffinity:
    required:
      nodeSelectorTerms:
        - matchExpressions:
            - key: kubernetes.io/hostname
              operator: In
              values:
                - "10.193.72.33"
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: fiobench-pvc
  namespace: mercado
  labels:
    fiobench: image
    deployment: test
    subject: separated-workload-from-volume
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1000Gi
  volumeName: fiobench-pv
  storageClassName: longhorn-static
  selector:
    matchLabels:
      fiobench: image
      deployment: test
      subject: separated-workload-from-volume
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fiobench-deployment
  namespace: mercado
  labels:
    fiobench: image
    deployment: test
    subject: separated-workload-from-volume
spec:
  replicas: 1
  selector:
    matchLabels:
      fiobench: image
      deployment: test
      subject: separated-workload-from-volume
  template:
    metadata:
      labels:
        fiobench: image
        deployment: test
        subject: separated-workload-from-volume
    spec:
      containers:
      - name: fiobench
        image: ghcr.io/pabloheigvd/tb-fiobench:latest
        command: [ "/bin/sh", "run-all-jobs.sh" ]
        volumeMounts:
          - name: fiobench-storage
            mountPath: /data
      nodeSelector:
        kubernetes.io/hostname: "10.193.72.32"
      volumes:
        - name: fiobench-storage
          persistentVolumeClaim:
            claimName: fiobench-pvc
